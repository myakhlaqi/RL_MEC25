{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8099ace3-3c5a-471a-bcf5-dc2aba13ea15",
   "metadata": {},
   "source": [
    "# Install Prerequisites: python3.8+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71505546-1607-46f5-aad8-0a364f8d8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3[extra]\n",
    "# # !pip install sb3-contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74e558-c973-40a4-8e3d-3f8210cef115",
   "metadata": {},
   "source": [
    "See Algorithms, Parameters and More:\n",
    "1. https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "2. https://sb3-contrib.readthedocs.io/en/master/index.html\n",
    "3. https://github.com/DLR-RM/stable-baselines3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2f42a-26f9-4dc1-8322-e5a40be8ed8b",
   "metadata": {},
   "source": [
    "# Define Custom Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab92dd00-4d93-4355-87be-316ffbce0a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csmas\\AppData\\Local\\Temp\\ipykernel_29808\\3919075687.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "import simpy\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "NUM_DATA = 1000\n",
    "SLOW_POWER = 1\n",
    "FAST_POWER = 10\n",
    "SLOW_RATE = 5\n",
    "FAST_RATE = 10\n",
    "ARRIVAL_RATE = 1        #np.random.exponential(1/ARRIVAL_RATE)\n",
    "MIN_LATENCY = 0.01\n",
    "MAX_LATENCY = 0.1\n",
    "MIN_COMPUTATION_REQ = 1\n",
    "MAX_COMPUTATION_REQ = 1000\n",
    "NUM_FAST_SERVERS = 2\n",
    "NUM_SLOW_SERVERS = 10\n",
    "NUM_SERVERS = NUM_FAST_SERVERS+NUM_SLOW_SERVERS\n",
    "PRIORITIES = 3\n",
    "COST_MUL = 0.0005 # multiplier of cost in Reward calculation\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, env, slow_capacity, fast_capacity, slow_computing_power, fast_computing_power, slow_rate, fast_rate):\n",
    "        self.env = env\n",
    "        self.slow_capacity = slow_capacity\n",
    "        self.fast_capacity = fast_capacity \n",
    "        self.slow_computing_power = slow_computing_power\n",
    "        self.fast_computing_power = fast_computing_power\n",
    "        self.slow_rate = slow_rate\n",
    "        self.fast_rate = fast_rate\n",
    "        self.slow_resource = simpy.PriorityResource(env, capacity=slow_capacity)\n",
    "        self.fast_resource = simpy.PriorityResource(env, capacity=fast_capacity)\n",
    "        \n",
    "    def process_packet(self, packet, completion_event):\n",
    "        if packet.server_assigned==\"fast\":\n",
    "            yield self.env.timeout(packet.latencies[0])\n",
    "            completion_event.succeed()\n",
    "            computation_time = packet.computational_requirement / self.fast_computing_power\n",
    "            packet.cost = packet.computational_requirement * self.fast_rate\n",
    "\n",
    "            with self.fast_resource.request(priority=packet.priority) as req:\n",
    "                yield req\n",
    "                packet.wait_end_time = self.env.now\n",
    "                yield self.env.timeout(computation_time)\n",
    "                \n",
    "        else:\n",
    "            yield self.env.timeout(packet.latencies[1])\n",
    "            completion_event.succeed()\n",
    "            computation_time = packet.computational_requirement / self.slow_computing_power\n",
    "            packet.cost = packet.computational_requirement * self.slow_rate\n",
    "            \n",
    "            with self.slow_resource.request(priority=packet.priority) as req:\n",
    "                yield req\n",
    "                packet.wait_end_time = self.env.now\n",
    "                yield self.env.timeout(computation_time)\n",
    "                \n",
    "                \n",
    "\n",
    "class Packet:\n",
    "    def __init__(self, env, name, id, creq, lats):\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.computational_requirement = creq  # Random computational requirement\n",
    "        self.latencies = lats  # Random latency from each server\n",
    "        self.wait_start_time = 0\n",
    "        self.wait_end_time = 0\n",
    "        self.wait = 0\n",
    "        self.done_time = 0\n",
    "        self.server_assigned = None\n",
    "        self.priority = 1\n",
    "        self.cost = 0\n",
    "    def _wait(self):\n",
    "        self.wait = self.wait_end_time - self.wait_start_time\n",
    "\"\"\"-----------------------------------------------------------------------------------------------------------\"\"\"\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    def __init__(self,alpha = 0.5):\n",
    "        super().__init__()\n",
    "        self.waits = 0\n",
    "        self.costs = 0\n",
    "        self.packets=[]\n",
    "        self.packet_id = 0\n",
    "        self.alpha = alpha # is the Î± is a parameter that determines the trade-off between energy consumption and task execution delay.\n",
    "\n",
    "        \n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self._action_code = {\n",
    "            0: [\"fast\", 1],\n",
    "            1: [\"fast\", 2],\n",
    "            2: [\"fast\", 3],\n",
    "            3: [\"slow\", 1],\n",
    "            4: [\"slow\", 2],\n",
    "            5: [\"slow\", 3]\n",
    "        }\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'comp_req'   : spaces.Box(low=MIN_COMPUTATION_REQ, high=MAX_COMPUTATION_REQ, shape=(1,), dtype=np.int32),\n",
    "            'latency'    : spaces.Box(low=MIN_LATENCY, high=MAX_LATENCY, shape=(2,), dtype=np.float32),\n",
    "            'queue_size' : spaces.Box(low=0, high=NUM_DATA, shape=(2,), dtype=np.int32),\n",
    "            'using'      : spaces.Box(low=0, high=max(NUM_FAST_SERVERS,NUM_SLOW_SERVERS), shape=(2,), dtype=np.int32),\n",
    "            'priorities' : spaces.Box(low=0, high=PRIORITIES*NUM_DATA, shape=(2,), dtype=np.int32),    \n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.waits = 0\n",
    "        self.costs = 0\n",
    "        self.packets=[]\n",
    "        self.packet_id = 0\n",
    "        \n",
    "        self.env = simpy.Environment()\n",
    "        self.server = Server(self.env, NUM_SLOW_SERVERS, NUM_FAST_SERVERS, SLOW_POWER, FAST_POWER, SLOW_RATE, FAST_RATE)\n",
    "        \n",
    "        self.current_state = self.observation_space.sample()\n",
    "        self.packet_id = 0\n",
    "        self.current_state['queue_size'] = np.array([len(self.server.fast_resource.queue), len(self.server.slow_resource.queue)], dtype=np.int32)\n",
    "        self.current_state['using'] = np.array([self.server.fast_resource.count, self.server.slow_resource.count], dtype=np.int32)\n",
    "        self.current_state['priorities'] = np.array([sum([i.priority if self.server.fast_resource.queue else 0 for i in self.server.fast_resource.queue]), sum([i.priority if self.server.slow_resource.queue else 0 for i in self.server.slow_resource.queue])], dtype=np.int32)\n",
    "        return self.current_state, {}\n",
    "\n",
    "    def _make_packet(self,action, id, state, completion_event):\n",
    "        packet = Packet( self.env, f'Packet_{id}', id, state['comp_req'], state['latency'])\n",
    "        packet.wait_start_time = self.env.now\n",
    "        [s, p] = self._action_code[action]\n",
    "        packet.server_assigned = s\n",
    "        packet.priority = p\n",
    "        yield self.env.process(self.server.process_packet(packet, completion_event))\n",
    "        packet.done_time = self.env.now\n",
    "        packet._wait()\n",
    "        self.waits += packet.wait\n",
    "        self.costs += packet.cost\n",
    "        self.packets.append(packet)\n",
    "        \n",
    "    def _packet_generator(self, action, id, state, completion_event):\n",
    "        yield self.env.timeout(np.random.exponential(1/ARRIVAL_RATE))\n",
    "        self.env.process(self._make_packet(action, id, state, completion_event))\n",
    "        \n",
    "    def step(self, action):\n",
    "        completion_event = self.env.event()\n",
    "        self.env.process(self._packet_generator(action, self.packet_id, self.current_state, completion_event))\n",
    "        self.env.run(until=completion_event)\n",
    "\n",
    "        self.current_state = self.observation_space.sample()\n",
    "        self.packet_id +=1\n",
    "        self.current_state['queue_size'] = np.array([len(self.server.fast_resource.queue), len(self.server.slow_resource.queue)], dtype=np.int32)\n",
    "        self.current_state['using'] = np.array([self.server.fast_resource.count, self.server.slow_resource.count], dtype=np.int32)\n",
    "        self.current_state['priorities'] = np.array([sum([i.priority if self.server.fast_resource.queue else 0 for i in self.server.fast_resource.queue]), sum([i.priority if self.server.slow_resource.queue else 0 for i in self.server.slow_resource.queue])], dtype=np.int32)\n",
    "\n",
    "        reward = 1\n",
    "        terminated = self.packet_id == NUM_DATA\n",
    "        if terminated:\n",
    "            self.env.run()\n",
    "            reward = 1/(self.waits/NUM_DATA) + COST_MUL/(self.costs/NUM_DATA)\n",
    "            # reward = - self.alpha * (self.waits) - (1 - self.alpha) * (self.costs)\n",
    "        \n",
    "        return self.current_state, reward, terminated, False, {}\n",
    "\n",
    "    def close(self):\n",
    "        del self.env\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f780b6-6c82-4330-ae81-65a1ea7ee3a1",
   "metadata": {},
   "source": [
    "## Check Environment for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5321c208-ff98-4d8e-8c23-ad648db484be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csmas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# from gymnasium.utils.env_checker import check_env\n",
    "env=CustomEnv()\n",
    "check_env(env, warn=True, skip_render_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad5cc5-ba8f-498e-8da6-4f5631b3f192",
   "metadata": {},
   "source": [
    "# Random Action Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6f1555-6b33-43de-8238-2ba5adeb950b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average waits: [8836.44422684]\n",
      "\n",
      "average cost: [3705.35464535]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env=CustomEnv()\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(NUM_DATA):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(f\"average waits: {env.waits/(env.packet_id+1)}\\n\")\n",
    "print(f\"average cost: {env.costs/(env.packet_id+1)}\\n\")\n",
    "\n",
    "env.close( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd829de-98b9-490a-9bae-2129c88da868",
   "metadata": {},
   "source": [
    "# Train RL Model & Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb0a4f8-35e0-4b7e-97d8-55c2bc1498a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csmas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 2048, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 800          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009318888 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -0.00226     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 6.99         |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 999          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 200          |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 1600         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029660612 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | -0.000319    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 5.82         |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    value_loss           | 12           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 999          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2400         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005195886 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 4.97         |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.00756     |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 999          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 400          |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 3200         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021851063 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -0.00142     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 4.1          |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | -0.0199      |\n",
      "|    value_loss           | 8.44         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 999          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 500          |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086600855 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.000991     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 3.36         |\n",
      "|    n_updates            | 4990         |\n",
      "|    policy_gradient_loss | -0.00987     |\n",
      "|    value_loss           | 6.89         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 999          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 78           |\n",
      "|    iterations           | 600          |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 4800         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048984066 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.00162      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 5990         |\n",
      "|    policy_gradient_loss | -0.0236      |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "average waits: [12115.67155093]\n",
      "\n",
      "average cost: [4975.81418581]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from sb3_contrib import TRPO\n",
    "\n",
    "env = CustomEnv(0.9)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=0.0001, n_steps=8, batch_size=2048)\n",
    "# model = DQN(\"MultiInputPolicy\", env, verbose=1, batch_size=32, target_update_interval=100)\n",
    "# model = A2C(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model = TRPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=5_000, log_interval=100) #30_000\n",
    "\n",
    "obs,_ = env.reset()\n",
    "for i in range(NUM_DATA):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, tru, info = env.step(int(action))\n",
    "    \n",
    "print(f\"average waits: {env.waits/(env.packet_id+1)}\\n\")\n",
    "print(f\"average cost: {env.costs/(env.packet_id+1)}\\n\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49032c-2969-41c9-b391-b717ab2a5a89",
   "metadata": {},
   "source": [
    "## Simulate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891d6637-ab28-4f9a-9cb4-0fbc2e522c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average waits: [11846.99384512]\n",
      "\n",
      "average cost: [4951.18881119]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "for i in range(NUM_DATA):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, tru, info = env.step(int(action))\n",
    "print(f\"average waits: {env.waits/(env.packet_id+1)}\\n\")\n",
    "print(f\"average cost: {env.costs/(env.packet_id+1)}\\n\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2614b-acc0-4d7a-ae8b-f27e31bab54d",
   "metadata": {},
   "source": [
    "# Save Packet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "047d4757-f47c-444f-88b3-e73f18312f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([e.__dict__ for e in env.packets]).to_csv(\"packets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
